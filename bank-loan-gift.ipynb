{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1089480,"sourceType":"datasetVersion","datasetId":608073}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries ","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\n\n\nfrom sklearn.feature_selection import mutual_info_classif\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:33.596025Z","iopub.execute_input":"2025-03-20T13:26:33.596620Z","iopub.status.idle":"2025-03-20T13:26:35.697320Z","shell.execute_reply.started":"2025-03-20T13:26:33.596570Z","shell.execute_reply":"2025-03-20T13:26:35.696287Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"def preprocess_data(file_path, drop_cols, target_col):\n\n    # Load dataset with only necessary columns\n    df = pd.read_csv(file_path)\n    print(f'Initial Data Shape: {df.shape}')\n\n    # Drop specified columns efficiently\n    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n\n    # Remove duplicate rows in place\n    print(f'Duplicate Rows: { df.duplicated().sum()}')\n    df.drop_duplicates(inplace=True)\n\n    print(f'Shape after dropping duplicates: {df.shape}')\n\n    # Standardize features \n    cols_std = [col for col in df.columns if col != target_col]\n    print(f'Num of features to standardize: {len(cols_std)}')\n\n    if cols_std:  \n        df[cols_std] = StandardScaler().fit_transform(df[cols_std])\n\n    # shuffle df\n    df = df.sample(frac=1).reset_index(drop=True)\n\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.698756Z","iopub.execute_input":"2025-03-20T13:26:35.699637Z","iopub.status.idle":"2025-03-20T13:26:35.706105Z","shell.execute_reply.started":"2025-03-20T13:26:35.699577Z","shell.execute_reply":"2025-03-20T13:26:35.704902Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def split_data(df, target_col, test_size=0.1, val_size=0.2):\n    X, y = df.drop(columns=[target_col]), df[target_col]\n    # split data  train val , test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, \n                                                        random_state=42, stratify=y)\n    \n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, \n                                                      random_state=42, stratify=y_train)\n\n    print(f\"Train Shape: {X_train.shape}, Validation Shape: {X_val.shape}, Test Shape: {X_test.shape}\")\n    target_sets = { \"Train\": y_train, \"Validation\": y_val, \"Test\": y_test }\n\n\n    for set_name, target in target_sets.items():\n        print(f\"\\nTarget Distribution in {set_name} Set:\")\n        print(target.value_counts(normalize=True))\n        \n    return X_train, X_val, X_test, y_train, y_val, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.708367Z","iopub.execute_input":"2025-03-20T13:26:35.708705Z","iopub.status.idle":"2025-03-20T13:26:35.737618Z","shell.execute_reply.started":"2025-03-20T13:26:35.708678Z","shell.execute_reply":"2025-03-20T13:26:35.736250Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\ndef evaluate_model(model, X_true, y_true):\n    \n    # predict class and probability \n    y_pred = model.predict(X_true)\n    y_pred_proba = model.predict_proba(X_true)\n    \n    # confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    \n    # metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0  # True Negative Rate\n\n    # Store metrics in a dictionary\n    metrics_dict = {\n        \"Accuracy\": accuracy,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1-Score\": f1,\n        \"Specificity\": specificity}\n      \n    print(f\"\\nConfusion Matrix:\\n{cm}\")\n    print(\"\\nEvaluation Metrics:\")\n    for key, value in metrics_dict.items():\n        if value is not None: print(f\"{key}: {value:.4f}\")\n    \n    return metrics_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.739184Z","iopub.execute_input":"2025-03-20T13:26:35.739564Z","iopub.status.idle":"2025-03-20T13:26:35.759387Z","shell.execute_reply.started":"2025-03-20T13:26:35.739521Z","shell.execute_reply":"2025-03-20T13:26:35.758212Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def mutual_information(df, target): \n    X, y = df.drop(columns=[target_col]), df[target_col]\n    mi_scores = mutual_info_classif(X, y)\n    feature_names = [i for i in X.columns]\n    \n    \n    # Convert to DataFrame for sorting\n    mi_df = pd.DataFrame({\"Feature\": feature_names, \"Mutual Information\": mi_scores})\n    mi_df = mi_df.sort_values(by=\"Mutual Information\", ascending=True)  # Sort for better visualization\n    \n    # Plot Horizontal Bar Chart\n    plt.figure(figsize=(10, 6))\n    plt.barh(mi_df[\"Feature\"], mi_df[\"Mutual Information\"], color=\"skyblue\")\n    plt.xlabel(\"Mutual Information Score\")\n    plt.ylabel(\"Features\")\n    plt.title(\"Mutual Information Scores for Features\")\n    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.760481Z","iopub.execute_input":"2025-03-20T13:26:35.760951Z","iopub.status.idle":"2025-03-20T13:26:35.787758Z","shell.execute_reply.started":"2025-03-20T13:26:35.760887Z","shell.execute_reply":"2025-03-20T13:26:35.786579Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def main_function(df, target_col, classifiers): \n    trained_models = {}\n    print(f' {\"*\" * 20} Train Test Split {\"*\" * 20}')\n    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df=df, target_col=target_col)\n    print(\"=\" * 60)\n\n    for name, classifier in classifiers.items():\n        classifier.fit(X_train, y_train)\n        trained_models[name]  = classifier\n        print(f\"\\n{classifier.__class__.__name__} Trained\")\n    \n        print(f' {\"*\" * 20} Validation Set {\"*\" * 20}')\n        evaluate_model(model = classifier, X_true=X_val, y_true=y_val)\n        print(f' {\"*\" * 20} Test Set {\"*\" * 20}')\n        evaluate_model(model = classifier, X_true = X_test, y_true=y_test)\n        print(\"=\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.788741Z","iopub.execute_input":"2025-03-20T13:26:35.789062Z","iopub.status.idle":"2025-03-20T13:26:35.811132Z","shell.execute_reply.started":"2025-03-20T13:26:35.789036Z","shell.execute_reply":"2025-03-20T13:26:35.809874Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Variables ","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/bank-personal-loan-modelling/Bank_Personal_Loan_Modelling.csv'\ndrop_cols = ['ID', 'ZIP Code']\ntarget_col = 'Personal Loan'\ngift_cols_target = ['Income', 'CCAvg', 'CD Account', 'Education', 'Mortgage', 'Family', 'Personal Loan']\nmu_i_cols_target = ['Income', 'CCAvg', 'CD Account', 'Education', 'Mortgage', 'Family', 'CreditCard', 'Personal Loan']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.812391Z","iopub.execute_input":"2025-03-20T13:26:35.812831Z","iopub.status.idle":"2025-03-20T13:26:35.836177Z","shell.execute_reply.started":"2025-03-20T13:26:35.812787Z","shell.execute_reply":"2025-03-20T13:26:35.835152Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_classifiers():\n    return {\n        \"SVM\": svm.SVC(probability=True),\n        \"Logistic Regression\": LogisticRegression(),\n        \"Random Forest\": RandomForestClassifier(),\n        \"Gradient Boosting\": GradientBoostingClassifier(),\n        \"LightGBM\": LGBMClassifier(),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.838484Z","iopub.execute_input":"2025-03-20T13:26:35.838840Z","iopub.status.idle":"2025-03-20T13:26:35.856209Z","shell.execute_reply.started":"2025-03-20T13:26:35.838801Z","shell.execute_reply":"2025-03-20T13:26:35.854901Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Main ","metadata":{}},{"cell_type":"markdown","source":"**Preprocessing**","metadata":{}},{"cell_type":"code","source":"df_pre_processed  = preprocess_data(file_path=file_path, \n                drop_cols=drop_cols, \n                target_col=target_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.857426Z","iopub.execute_input":"2025-03-20T13:26:35.857768Z","iopub.status.idle":"2025-03-20T13:26:35.915517Z","shell.execute_reply.started":"2025-03-20T13:26:35.857737Z","shell.execute_reply":"2025-03-20T13:26:35.914298Z"}},"outputs":[{"name":"stdout","text":"Initial Data Shape: (5000, 14)\nDuplicate Rows: 13\nShape after dropping duplicates: (4987, 12)\nNum of features to standardize: 11\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"df_gift = df_pre_processed[gift_cols_target]\nprint(f'GIFT cols: {df_gift.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.916719Z","iopub.execute_input":"2025-03-20T13:26:35.917131Z","iopub.status.idle":"2025-03-20T13:26:35.924004Z","shell.execute_reply.started":"2025-03-20T13:26:35.917097Z","shell.execute_reply":"2025-03-20T13:26:35.922998Z"}},"outputs":[{"name":"stdout","text":"GIFT cols: (4987, 7)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"df_mui = df_pre_processed[mu_i_cols_target]\nprint(f'GIFT cols: {df_mui.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.925095Z","iopub.execute_input":"2025-03-20T13:26:35.925390Z","iopub.status.idle":"2025-03-20T13:26:35.941283Z","shell.execute_reply.started":"2025-03-20T13:26:35.925354Z","shell.execute_reply":"2025-03-20T13:26:35.940241Z"}},"outputs":[{"name":"stdout","text":"GIFT cols: (4987, 8)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## **ML models withOUT feature selection**","metadata":{}},{"cell_type":"code","source":"classifiers = get_classifiers()\nmain_function(df=df_pre_processed, target_col = target_col, classifiers=classifiers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:35.942206Z","iopub.execute_input":"2025-03-20T13:26:35.942481Z","iopub.status.idle":"2025-03-20T13:26:37.608153Z","shell.execute_reply.started":"2025-03-20T13:26:35.942458Z","shell.execute_reply":"2025-03-20T13:26:37.607054Z"}},"outputs":[{"name":"stdout","text":" ******************** Train Test Split ********************\nTrain Shape: (3590, 11), Validation Shape: (898, 11), Test Shape: (499, 11)\n\nTarget Distribution in Train Set:\nPersonal Loan\n0    0.903621\n1    0.096379\nName: proportion, dtype: float64\n\nTarget Distribution in Validation Set:\nPersonal Loan\n0    0.904232\n1    0.095768\nName: proportion, dtype: float64\n\nTarget Distribution in Test Set:\nPersonal Loan\n0    0.903808\n1    0.096192\nName: proportion, dtype: float64\n============================================================\n\nSVC Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[809   3]\n [ 17  69]]\n\nEvaluation Metrics:\nAccuracy: 0.9777\nPrecision: 0.9583\nRecall: 0.8023\nF1-Score: 0.8734\nSpecificity: 0.9963\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[450   1]\n [  9  39]]\n\nEvaluation Metrics:\nAccuracy: 0.9800\nPrecision: 0.9750\nRecall: 0.8125\nF1-Score: 0.8864\nSpecificity: 0.9978\n============================================================\n\nLogisticRegression Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[795  17]\n [ 30  56]]\n\nEvaluation Metrics:\nAccuracy: 0.9477\nPrecision: 0.7671\nRecall: 0.6512\nF1-Score: 0.7044\nSpecificity: 0.9791\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[447   4]\n [ 13  35]]\n\nEvaluation Metrics:\nAccuracy: 0.9659\nPrecision: 0.8974\nRecall: 0.7292\nF1-Score: 0.8046\nSpecificity: 0.9911\n============================================================\n\nRandomForestClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[811   1]\n [ 11  75]]\n\nEvaluation Metrics:\nAccuracy: 0.9866\nPrecision: 0.9868\nRecall: 0.8721\nF1-Score: 0.9259\nSpecificity: 0.9988\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[449   2]\n [  2  46]]\n\nEvaluation Metrics:\nAccuracy: 0.9920\nPrecision: 0.9583\nRecall: 0.9583\nF1-Score: 0.9583\nSpecificity: 0.9956\n============================================================\n\nGradientBoostingClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[808   4]\n [ 10  76]]\n\nEvaluation Metrics:\nAccuracy: 0.9844\nPrecision: 0.9500\nRecall: 0.8837\nF1-Score: 0.9157\nSpecificity: 0.9951\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[449   2]\n [  3  45]]\n\nEvaluation Metrics:\nAccuracy: 0.9900\nPrecision: 0.9574\nRecall: 0.9375\nF1-Score: 0.9474\nSpecificity: 0.9956\n============================================================\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 346, number of negative: 3244\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000271 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 616\n[LightGBM] [Info] Number of data points in the train set: 3590, number of used features: 11\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096379 -> initscore=-2.238124\n[LightGBM] [Info] Start training from score -2.238124\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\nLGBMClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[811   1]\n [  8  78]]\n\nEvaluation Metrics:\nAccuracy: 0.9900\nPrecision: 0.9873\nRecall: 0.9070\nF1-Score: 0.9455\nSpecificity: 0.9988\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[450   1]\n [  3  45]]\n\nEvaluation Metrics:\nAccuracy: 0.9920\nPrecision: 0.9783\nRecall: 0.9375\nF1-Score: 0.9574\nSpecificity: 0.9978\n============================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## **ML models with feature selection -- Mutual Infrmation**","metadata":{}},{"cell_type":"code","source":"classifiers_gift = get_classifiers() \nmain_function(df=df_mui, target_col = target_col, classifiers=classifiers_gift)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:37.609033Z","iopub.execute_input":"2025-03-20T13:26:37.609291Z","iopub.status.idle":"2025-03-20T13:26:38.988230Z","shell.execute_reply.started":"2025-03-20T13:26:37.609271Z","shell.execute_reply":"2025-03-20T13:26:38.987238Z"}},"outputs":[{"name":"stdout","text":" ******************** Train Test Split ********************\nTrain Shape: (3590, 7), Validation Shape: (898, 7), Test Shape: (499, 7)\n\nTarget Distribution in Train Set:\nPersonal Loan\n0    0.903621\n1    0.096379\nName: proportion, dtype: float64\n\nTarget Distribution in Validation Set:\nPersonal Loan\n0    0.904232\n1    0.095768\nName: proportion, dtype: float64\n\nTarget Distribution in Test Set:\nPersonal Loan\n0    0.903808\n1    0.096192\nName: proportion, dtype: float64\n============================================================\n\nSVC Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[808   4]\n [ 16  70]]\n\nEvaluation Metrics:\nAccuracy: 0.9777\nPrecision: 0.9459\nRecall: 0.8140\nF1-Score: 0.8750\nSpecificity: 0.9951\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[447   4]\n [  7  41]]\n\nEvaluation Metrics:\nAccuracy: 0.9780\nPrecision: 0.9111\nRecall: 0.8542\nF1-Score: 0.8817\nSpecificity: 0.9911\n============================================================\n\nLogisticRegression Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[798  14]\n [ 28  58]]\n\nEvaluation Metrics:\nAccuracy: 0.9532\nPrecision: 0.8056\nRecall: 0.6744\nF1-Score: 0.7342\nSpecificity: 0.9828\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[447   4]\n [ 15  33]]\n\nEvaluation Metrics:\nAccuracy: 0.9619\nPrecision: 0.8919\nRecall: 0.6875\nF1-Score: 0.7765\nSpecificity: 0.9911\n============================================================\n\nRandomForestClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[810   2]\n [ 10  76]]\n\nEvaluation Metrics:\nAccuracy: 0.9866\nPrecision: 0.9744\nRecall: 0.8837\nF1-Score: 0.9268\nSpecificity: 0.9975\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[449   2]\n [  3  45]]\n\nEvaluation Metrics:\nAccuracy: 0.9900\nPrecision: 0.9574\nRecall: 0.9375\nF1-Score: 0.9474\nSpecificity: 0.9956\n============================================================\n\nGradientBoostingClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[809   3]\n [  8  78]]\n\nEvaluation Metrics:\nAccuracy: 0.9878\nPrecision: 0.9630\nRecall: 0.9070\nF1-Score: 0.9341\nSpecificity: 0.9963\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[450   1]\n [  3  45]]\n\nEvaluation Metrics:\nAccuracy: 0.9920\nPrecision: 0.9783\nRecall: 0.9375\nF1-Score: 0.9574\nSpecificity: 0.9978\n============================================================\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 346, number of negative: 3244\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000232 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 516\n[LightGBM] [Info] Number of data points in the train set: 3590, number of used features: 7\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096379 -> initscore=-2.238124\n[LightGBM] [Info] Start training from score -2.238124\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\nLGBMClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[808   4]\n [ 11  75]]\n\nEvaluation Metrics:\nAccuracy: 0.9833\nPrecision: 0.9494\nRecall: 0.8721\nF1-Score: 0.9091\nSpecificity: 0.9951\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[449   2]\n [  4  44]]\n\nEvaluation Metrics:\nAccuracy: 0.9880\nPrecision: 0.9565\nRecall: 0.9167\nF1-Score: 0.9362\nSpecificity: 0.9956\n============================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## **ML models with feature selection -- GIFT**","metadata":{}},{"cell_type":"code","source":"classifiers_mi = get_classifiers()\nmain_function(df=df_gift, target_col = target_col, classifiers=classifiers_mi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T13:26:38.989221Z","iopub.execute_input":"2025-03-20T13:26:38.989495Z","iopub.status.idle":"2025-03-20T13:26:40.290545Z","shell.execute_reply.started":"2025-03-20T13:26:38.989471Z","shell.execute_reply":"2025-03-20T13:26:40.289524Z"}},"outputs":[{"name":"stdout","text":" ******************** Train Test Split ********************\nTrain Shape: (3590, 6), Validation Shape: (898, 6), Test Shape: (499, 6)\n\nTarget Distribution in Train Set:\nPersonal Loan\n0    0.903621\n1    0.096379\nName: proportion, dtype: float64\n\nTarget Distribution in Validation Set:\nPersonal Loan\n0    0.904232\n1    0.095768\nName: proportion, dtype: float64\n\nTarget Distribution in Test Set:\nPersonal Loan\n0    0.903808\n1    0.096192\nName: proportion, dtype: float64\n============================================================\n\nSVC Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[809   3]\n [ 14  72]]\n\nEvaluation Metrics:\nAccuracy: 0.9811\nPrecision: 0.9600\nRecall: 0.8372\nF1-Score: 0.8944\nSpecificity: 0.9963\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[448   3]\n [  7  41]]\n\nEvaluation Metrics:\nAccuracy: 0.9800\nPrecision: 0.9318\nRecall: 0.8542\nF1-Score: 0.8913\nSpecificity: 0.9933\n============================================================\n\nLogisticRegression Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[794  18]\n [ 30  56]]\n\nEvaluation Metrics:\nAccuracy: 0.9465\nPrecision: 0.7568\nRecall: 0.6512\nF1-Score: 0.7000\nSpecificity: 0.9778\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[447   4]\n [ 15  33]]\n\nEvaluation Metrics:\nAccuracy: 0.9619\nPrecision: 0.8919\nRecall: 0.6875\nF1-Score: 0.7765\nSpecificity: 0.9911\n============================================================\n\nRandomForestClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[809   3]\n [ 10  76]]\n\nEvaluation Metrics:\nAccuracy: 0.9855\nPrecision: 0.9620\nRecall: 0.8837\nF1-Score: 0.9212\nSpecificity: 0.9963\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[449   2]\n [  3  45]]\n\nEvaluation Metrics:\nAccuracy: 0.9900\nPrecision: 0.9574\nRecall: 0.9375\nF1-Score: 0.9474\nSpecificity: 0.9956\n============================================================\n\nGradientBoostingClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[809   3]\n [  9  77]]\n\nEvaluation Metrics:\nAccuracy: 0.9866\nPrecision: 0.9625\nRecall: 0.8953\nF1-Score: 0.9277\nSpecificity: 0.9963\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[450   1]\n [  2  46]]\n\nEvaluation Metrics:\nAccuracy: 0.9940\nPrecision: 0.9787\nRecall: 0.9583\nF1-Score: 0.9684\nSpecificity: 0.9978\n============================================================\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 346, number of negative: 3244\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 513\n[LightGBM] [Info] Number of data points in the train set: 3590, number of used features: 6\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096379 -> initscore=-2.238124\n[LightGBM] [Info] Start training from score -2.238124\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\nLGBMClassifier Trained\n ******************** Validation Set ********************\n\nConfusion Matrix:\n[[810   2]\n [ 11  75]]\n\nEvaluation Metrics:\nAccuracy: 0.9855\nPrecision: 0.9740\nRecall: 0.8721\nF1-Score: 0.9202\nSpecificity: 0.9975\n ******************** Test Set ********************\n\nConfusion Matrix:\n[[449   2]\n [  4  44]]\n\nEvaluation Metrics:\nAccuracy: 0.9880\nPrecision: 0.9565\nRecall: 0.9167\nF1-Score: 0.9362\nSpecificity: 0.9956\n============================================================\n","output_type":"stream"}],"execution_count":14}]}